{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, I am using the Fitbit Kaggle dataset and load it to my personal Bigquery in order to make the data cleaning process. \n\nI could use plain python/pandas code, but I want to show what I can do with SQL and BigQuery. ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-03T14:13:29.772801Z","iopub.execute_input":"2022-03-03T14:13:29.773716Z","iopub.status.idle":"2022-03-03T14:13:29.789882Z","shell.execute_reply.started":"2022-03-03T14:13:29.773664Z","shell.execute_reply":"2022-03-03T14:13:29.789295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the kaggle dataset to Bigquery\n\nThis step is not needed, since I could clean and analyze the dataset in python, but I want to play with the new tools that I learned. Therefore I will upload the kaggle dataset into my personal Google Cloud Services (GCS) area. \n\nReminder: link your Kaggle notebook to Google Cloud Services in the menu `Add-ons > Google Cloud Services`.\n\nI have a test project in my GCS area called `test-project-306614` where I will upload the datasets. ","metadata":{}},{"cell_type":"code","source":"from google.cloud import bigquery\n\nprojectID = 'test-project-306614'\ndatasetID = 'Fitabase'\n# Construct a BigQuery client object.\nclient = bigquery.Client(project=projectID, location='US')  ## not sure if the location is needed\n# client.create_dataset(datasetID)  ## to run only one time to create the dataset","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:13:29.791535Z","iopub.execute_input":"2022-03-03T14:13:29.791948Z","iopub.status.idle":"2022-03-03T14:13:29.799278Z","shell.execute_reply.started":"2022-03-03T14:13:29.791916Z","shell.execute_reply":"2022-03-03T14:13:29.798497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The next cell creates tables in the `datasetID`. It should be run only once. ","metadata":{}},{"cell_type":"code","source":"from google.api_core.exceptions import AlreadyExists, Conflict\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         tableName = filename.replace('_merged.csv','')\n#         try: client.create_table(f\"{projectID}.{datasetID}.{tableName}\")\n#         except Conflict: print(f'Table {tableName} already created')\n        \n#         datasetRef = client.dataset(datasetID)\n#         tableRef = datasetRef.table(tableName)\n#         jobConfig = bigquery.LoadJobConfig()\n        \n#         #### the fitbit dataset contains date columns using am/pm format, which BigQuery \n#         #### has problems identifying. I am using a quick hack using pandas here.\n#         tmpDf = pd.read_csv(os.path.join(dirname, filename))\n#         for iCol in tmpDf.columns.to_list():\n#             if iCol.endswith(('Minute', 'Hour', 'date', 'Time', 'Date', 'Day')): \n#                 tmpDf[iCol] = pd.to_datetime(tmpDf[iCol])\n#         job = client.load_table_from_dataframe(tmpDf, tableRef, job_config=jobConfig)\n#         job.result()\n        \n#         print(\"Loaded {} rows into {}:{}.\".format(job.output_rows, datasetID, tableName))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:13:29.800456Z","iopub.execute_input":"2022-03-03T14:13:29.80085Z","iopub.status.idle":"2022-03-03T14:13:29.814077Z","shell.execute_reply.started":"2022-03-03T14:13:29.80082Z","shell.execute_reply":"2022-03-03T14:13:29.81317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After creating the dataset in BigQuery, let's load the datasets for cleaning.","metadata":{}},{"cell_type":"code","source":"dataset = client.get_dataset(datasetID)   \ntables = list(client.list_tables(dataset))\n\n# Print names of all tables in the dataset\nfor table in tables: print(table.table_id)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:13:29.815986Z","iopub.execute_input":"2022-03-03T14:13:29.816708Z","iopub.status.idle":"2022-03-03T14:13:30.874932Z","shell.execute_reply.started":"2022-03-03T14:13:29.816668Z","shell.execute_reply":"2022-03-03T14:13:30.874098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Simple test to see if the queries are working:","metadata":{}},{"cell_type":"code","source":"query = f\"\"\" SELECT *\n        FROM `{projectID}.{datasetID}.dailyActivity`\"\"\"\n\n# Set up the query\nquery_job = client.query(query)\n\n# API request - run the query, and return a pandas DataFrame\ndata = query_job.to_dataframe()\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:13:30.876715Z","iopub.execute_input":"2022-03-03T14:13:30.877237Z","iopub.status.idle":"2022-03-03T14:13:32.76975Z","shell.execute_reply.started":"2022-03-03T14:13:30.877189Z","shell.execute_reply":"2022-03-03T14:13:32.768847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking the number of dates per Id.","metadata":{"execution":{"iopub.status.busy":"2022-03-02T18:29:45.107928Z","iopub.execute_input":"2022-03-02T18:29:45.108188Z","iopub.status.idle":"2022-03-02T18:29:45.114243Z","shell.execute_reply.started":"2022-03-02T18:29:45.108161Z","shell.execute_reply":"2022-03-02T18:29:45.11281Z"}}},{"cell_type":"code","source":"query = f\"\"\" \nSELECT \n    CAST(Id AS INT) AS newId,\n    COUNT(ActivityDate) as count\nFROM `{projectID}.{datasetID}.dailyActivity`\nGROUP BY newId\nORDER BY count ASC\n\"\"\"\n\nquery_job = client.query(query)\ndata = query_job.to_dataframe()\ndata","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:13:32.771309Z","iopub.execute_input":"2022-03-03T14:13:32.771806Z","iopub.status.idle":"2022-03-03T14:13:33.811664Z","shell.execute_reply.started":"2022-03-03T14:13:32.771759Z","shell.execute_reply":"2022-03-03T14:13:33.810813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt \n\nquery = f\"\"\" \nSELECT \n    CAST(Id AS STRING) AS newId, \n    CAST(ActivityDate AS DATE) AS Date\nFROM `{projectID}.{datasetID}.dailyActivity`\n\"\"\"\nquery_job = client.query(query)\ndata = query_job.to_dataframe()\nplt.figure(figsize=(10,10))\nsns.scatterplot(data=data, x='Date', y='newId' )","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:13:33.813175Z","iopub.execute_input":"2022-03-03T14:13:33.813665Z","iopub.status.idle":"2022-03-03T14:13:35.302981Z","shell.execute_reply.started":"2022-03-03T14:13:33.813618Z","shell.execute_reply":"2022-03-03T14:13:35.302158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's create tables without the ids 4057192912, 2347167796, 8253242879, 3372868164, and according to the plot above between `2016-04-12` and `2016-05-07` where all the participants have data.","metadata":{}},{"cell_type":"code","source":"# from google.api_core.exceptions import AlreadyExists, Conflict\n\n# for table in tables: \n#     print('Processing table: ', table.table_id)\n#     tableName = table.table_id+'_cleaned'\n#         try: client.create_table(f\"{projectID}.{datasetID}.{tableName}\")\n#         except Conflict: print(f'Table {tableName} already created')\n        \n#         datasetRef = client.dataset(datasetID)\n#         tableRef = datasetRef.table(tableName)\n#         jobConfig = bigquery.LoadJobConfig()\n        \n#         query = f\"\"\" \n# SELECT * \n# FROM `{projectID}.{datasetID}.{table.table_id}`\n# WHERE ActivityDa\n# \"\"\"\n# query_job = client.query(query)\n# data = query_job.to_dataframe()\n        \n# #         #### the fitbit dataset contains date columns using am/pm format, which BigQuery \n# #         #### has problems identifying. I am using a quick hack using pandas here.\n# #         tmpDf = pd.read_csv(os.path.join(dirname, filename))\n# #         for iCol in tmpDf.columns.to_list():\n# #             if iCol.endswith(('Minute', 'Hour', 'date', 'Time', 'Date', 'Day')): \n# #                 tmpDf[iCol] = pd.to_datetime(tmpDf[iCol])\n# #         job = client.load_table_from_dataframe(tmpDf, tableRef, job_config=jobConfig)\n# #         job.result()\n        \n# #         print(\"Loaded {} rows into {}:{}.\".format(job.output_rows, datasetID, tableName))","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:13:35.304639Z","iopub.execute_input":"2022-03-03T14:13:35.305195Z","iopub.status.idle":"2022-03-03T14:13:35.312048Z","shell.execute_reply.started":"2022-03-03T14:13:35.30515Z","shell.execute_reply":"2022-03-03T14:13:35.310969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking if the tables have `nan` values in there","metadata":{}},{"cell_type":"code","source":"for table in tables: \n    print('Checking table: ', table.table_id)\n    query = f\"\"\"\nSELECT col_name, COUNT(1) AS null_count\nFROM `{projectID}.{datasetID}.{table.table_id}` AS t,\nUNNEST(REGEXP_EXTRACT_ALL(TO_JSON_STRING(t), r'\"(\\w+)\":null')) col_name\nGROUP BY col_name\n\"\"\"\n\n    query_job = client.query(query)\n    data = query_job.to_dataframe()\n    display(data)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:13:35.313639Z","iopub.execute_input":"2022-03-03T14:13:35.313983Z","iopub.status.idle":"2022-03-03T14:13:56.162432Z","shell.execute_reply.started":"2022-03-03T14:13:35.313925Z","shell.execute_reply":"2022-03-03T14:13:56.161582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = f\"\"\" \nSELECT *\nFROM `{projectID}.{datasetID}.weightLogInfo`\nWHERE Fat > 0\n\"\"\"\n\nquery_job = client.query(query)\ndata = query_job.to_dataframe()\ndata","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:13:56.164648Z","iopub.execute_input":"2022-03-03T14:13:56.164897Z","iopub.status.idle":"2022-03-03T14:13:56.223006Z","shell.execute_reply.started":"2022-03-03T14:13:56.164866Z","shell.execute_reply":"2022-03-03T14:13:56.22185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking if there are duplicate rows in tables:","metadata":{}},{"cell_type":"code","source":"for table in tables: \n    print('Checking table: ', table.table_id)\n    query = f\"\"\" \nSELECT\n    (SELECT COUNT(1) FROM (SELECT DISTINCT * FROM `{projectID}.{datasetID}.{table.table_id}`)) AS distinct_rows,\n    (SELECT COUNT(1) FROM `{projectID}.{datasetID}.{table.table_id}`) AS total_rows\n\"\"\"\n    query_job = client.query(query)\n    data = query_job.to_dataframe()\n    display(data)","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:13:56.224097Z","iopub.status.idle":"2022-03-03T14:13:56.224669Z","shell.execute_reply.started":"2022-03-03T14:13:56.224472Z","shell.execute_reply":"2022-03-03T14:13:56.224494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, just for fun, I merged all the daily information into one `csv` file.","metadata":{}},{"cell_type":"code","source":"query = f\"\"\"\nWITH dailyActivity AS (\n    SELECT \n        dailyActivity.*,\n        dailySleep.* EXCEPT( Id, SleepDay )\n    FROM `{projectID}.{datasetID}.dailyActivity` AS dailyActivity\n    LEFT JOIN `{projectID}.{datasetID}.sleepDay` AS dailySleep \n        ON dailyActivity.Id = dailySleep.Id AND CAST( dailyActivity.ActivityDate as dateTime ) = CAST( dailySleep.SleepDay as dateTime )\n)\nSELECT \n    dailyActivity.*,\n    dailyWeight.* EXCEPT(Id, Date)\nFROM dailyActivity\nLEFT JOIN `{projectID}.{datasetID}.weightLogInfo` AS dailyWeight \n    ON dailyActivity.Id = dailyWeight.Id AND CAST( dailyActivity.ActivityDate as dateTime ) = CAST( dailyWeight.Date as dateTime )\n#GROUP BY Id\n\"\"\"\n\n# Set up the query\nquery_job = client.query(query)\n\n# API request - run the query, and return a pandas DataFrame\ndata = query_job.to_dataframe()\ndata.to_csv('dailyActivity_allmerged.csv', index=False )\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-03T14:13:56.225838Z","iopub.status.idle":"2022-03-03T14:13:56.226438Z","shell.execute_reply.started":"2022-03-03T14:13:56.226239Z","shell.execute_reply":"2022-03-03T14:13:56.226262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}